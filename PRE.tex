\documentclass{article}[12pt]

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}      
\usepackage[english]{babel}
%\usepackage[left=2.00cm, right=2cm, top=2.00cm, bottom=2.00cm]{geometry}
\usepackage{amsthm}
\usepackage{fancyhdr}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{mathrsfs}
\usepackage{stmaryrd} 
%\renewcommand{\qedsymbol}{}
\usepackage{graphicx}
\graphicspath{{illustrations/}}
\usepackage{caption}
\usepackage{dsfont}
\usepackage{xcolor}
\usepackage{textcomp}

\pagestyle{fancy}
\renewcommand{\headrulewidth}{1pt}
\lhead{}
\chead{\textsc{numerical simulation of sdes with distributional drift}}
\rhead{}

\newcommand{\pade}[2]{\frac{\partial #1}{\partial #2}}
\newcommand{\de}[2]{\frac{\mathrm{d} #1}{\mathrm{d} #2}}
\newcommand{\integ}[4]{\int_{#1}^{#2}{#3\ \mathrm{d} #4}}

\newtheorem{defi}{Definition}
\newtheorem{theo}{Theorem}
\newtheorem{lem}[theo]{Lemma}
\newtheorem{cor}[theo]{Corollary}
\newtheorem{Pro}{Proposition}
\newtheorem{ex}{Example}
\newtheorem{rem}{Remark}

\newcommand{\norme}[1]{\left\Vert #1\right\Vert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\D}{D(\Omega)}
\newcommand{\Di}{D'(\Omega)}
\newcommand{\C}{\mathbb{C}}
\newcommand{\Cinf}{\mathcal{C}^\infty(\Omega,\R)}
\newcommand\gts[1]{\og#1\fg}
\newcommand{\n}{n\in\N^*}
\newcommand{\di}{\mathrm{d}}
\newcommand{\loc}{\mathcal{L}_{loc}^{1}(\Omega)}

\begin{document}
\title{\Large \textsc{\textbf{numerical simulation of sdes with distributional drift}}} \author{Maximilien \textsc{Germain} \date{May 2018}}
\maketitle

\section{Introduction}

    \paragraph{}
    We would like to simulate numerically sample paths of the solution of the stochastic differential equation
    \begin{equation} \label{sde}
    \di X_t = b(X_t)\ \di t + \di W_t
    \end{equation}
    where $b\in H^s_q(\R),\ s\in\left]-\frac{1}{2},0\right[$, $t\in[0,T]$, and $W_t$ is a standard Brownian motion. This equation is studied by F. Flandoli, E. Issoglio, and F. Russo in \cite{Fla-Iss-Rus-2017} in which they define a virtual solution for equation (\ref{sde}). The authors prove then existence and unicity in law of this solution. 
    
    \begin{ex}
        An example of such drift $b$ is given by the derivative of a sample path of a fractional Brownian motion $B^H_x$ with Hurst index $1/2<H<1$. These stochastic processes are gaussian processes verifying $$\E\left[B_t^HB_s^H\right]=\frac{1}{2}\left(t^{2H}+s^{2H}+|t-s|^{2H}\right).$$ We note $s = H - 1$. Given $B^H_x(\omega)\in H^{s+1}_q(\R)$, we can take $b(x) = \pade{}{x}B^H_x(\omega)\in H^s_q(\R)$. We will use this in our numerical simulations.
    \end{ex}    
    
    \paragraph{}
    As far as the drift $b$ is not a function but a distribution, it must be approximated if we want to evaluate it at points. In order to do so, we will use a series representation of $b$ and truncate it. That is why we will consider two steps in our algorithm: \begin{enumerate}
        \item approximate the drift $b$ by a function $b^N$ meant to converge to $b$ as $N\rightarrow\infty$.
        \item approximate the solution $X^N_t$ of the approximated SDE
        \begin{equation} \label{sde2}
        \di X^N_t = b^N\left(X^N_t\right)\ \di t + \di W_t
        \end{equation} 
        by $X^{N,n}_t$ defined with the Euler-Maruyama scheme
        \begin{equation*}
        X^{N,n}_t = X_0 + \int_0^t b^N\left(X^{N,n}_{\eta_n(t)}\right)\di t + W_{\eta_n(t)}
        \end{equation*}
        where $\eta_n(t)=t_k$ if $t\in[t_k,t_k+1]$, for $t_k=\frac{k}{n}$ with $ k\in\llbracket0,\lceil2^nT\rceil\rrbracket$.
        
    \end{enumerate}

\section{Numerical simulation of fractional Brownian motion}    
    \paragraph{}
    To simulate a sample path of a fractional brownian motion $B^H_x$ on a finite grid $(x_k)_{k\in\llbracket1,n\rrbracket}$, we simulate $n$ independent standard gaussian random variables $(X_k)_{k\in\llbracket1,n\rrbracket}$ and then correlate them with the definite positive correlation matrix 
    $$C_{k,s}=\E\left[B_{x_k}^HB_{x_s}^H\right]=\frac{1}{2}\left(x_k^{2H}+x_s^{2H}+|x_k-x_s|^{2H}\right).$$
    To do so, we use the Cholesky decomposition method and calculate the triangular matrix $M$ such that $C=MM^\top$. Therefore, defining
    $$X = \begin{pmatrix}
    X_1 \\ \vdots \\ X_n
    \end{pmatrix}\ \mathrm{and}\ B^H = MX,$$
    $B^H$ contains the values of a fractional brownian motion evaluated on the grid $(x_k)_{k\in\llbracket1,n\rrbracket}$.
    
\section{Approximation of the drift}
    \subsection{Series representation}
    \paragraph{}
    We use Haar wavelets to give a series representation of $b$. By doing so, we will be able to approximate it numerically by truncating the series.
    
    \begin{defi}[Haar wavelets]
        We define the Haar wavelets $h_{j,m}$ on $\R$ with $j\in\N\cup\{-1\}$ and $m\in\Z$ by:
        $$\begin{cases}
        h_M&:x\longmapsto\left(\mathds{1}_{\left[0,\frac{1}{2}\right[}-\mathds{1}_{\left[\frac{1}{2},1\right[}\right)(x)\\ h_{-1,m}&:x\longmapsto\sqrt{2}|h_M(x-m)|\\
        h_{j,m}&:x\longmapsto h_M(2^jx-m) 
        \end{cases}$$
    \end{defi}
    
    \begin{theo}[See \cite{Iss-Rus-2200}]
        Let $b\in H^s_q(\R)$ for $2\leq q \leq \infty$, and $s\in\left]-\frac{1}{2},\frac{1}{q}\right[$. Therefore,
        \begin{equation}
        b = \sum_{j=-1}^{+\infty}\sum_{m\in\Z}\mu_{j,m}h_{j,m}
        \end{equation}
        where $\mu_{j,m} = 2^j\int_{\R}b(x)h_{j,m}(x)\ \di x$ in the sense of dual pairing.
    \end{theo}

    \begin{defi}
        With the same notation $\mu_{j,m} $, let $b\in H^s_q(\R)$ for $2\leq q \leq \infty$, and $s\in\left]-\frac{1}{2},\frac{1}{q}\right[$. Given $N\in\N^*$ we define $b^N$ by:
        \begin{equation}
        b^N = \sum_{m=N}^{N-1}\mu_{-1,m}h_{-1,m}+\sum_{j=0}^{N}\sum_{m=-N2^j}^{N2^j-1}\mu_{j,m}h_{j,m}.
        \end{equation}
    \end{defi}

    \begin{rem}
        We can note that $\mathrm{Supp}\ b^N\subset [-N,N].$ Moreover, we have: $$\norme{b-b^N}_{H_q^s(\R)} \underset{N\rightarrow+\infty}{\longrightarrow} 0.$$
    \end{rem}

%    \subsection{Computation of the coefficients $\mu_{j,m}$ when $b$ is the derivative of a fractional brownian motion}
%        Faber basis
%\section{Numerical results}
%
%\newpage
\section{Convergence} 
    
    %\subsection{Weak xonvergence of $X_s^{N,n}$ to $X_s^N$}
    
    %\newpage
    
    \subsection{Weak convergence of $X^{N}$ to $X$}
        \paragraph{}
        We want to estimate the weak error $\E\left[f\left(X_T\right)-f\left(X_T^N\right)\right]$ with suitable functions $f$. In order to do so, we must go back to the definition of the virtual solution of the SDE (\ref{sde}) given in \cite{Fla-Iss-Rus-2017}. The authors define the virtual solution of SDE (\ref{sde}) by $X_t$ such that:
        \begin{equation}
        \begin{cases}
        X_t = \Psi(t,Y_t) = \varphi^{-1}(t,Y_t)
        \\
        Y_t = y + (\lambda+1)\int_0^t u(s,Y_s)\ \di s +\int_0^t (\nabla u(s,Y_s)+1)\ \di W_s
        \end{cases}
        \end{equation}
        where $u$ is the mild solution in $H_p^{1+\delta}$ of the following parabolic PDE:
        \begin{equation}\label{pde}
        \begin{cases}
        \partial_t u + \frac{1}{2}\Delta u + b\nabla u - (\lambda+1)u = -b\ &\mathrm{on}\ [0,T]\times\R\\
        u(T) = 0\ &\mathrm{on}\ \R
        \end{cases}
        \end{equation}
        and $\varphi(t,x) = x + u(t,x)$.
        
        \paragraph{}
        We also define another similar PDE by replacing $b$ by $b^N$. We call $u^N$ its mild solution in $H_p^{1+\delta}$.
        \begin{equation}\label{pde2}
        \begin{cases}
        \partial_t u^N + \frac{1}{2}\Delta u^N + b^N\nabla u^N - (\lambda+1)u^N = -b^N\ &\mathrm{on}\ [0,T]\times\R\\
        u^N(T) = 0\ &\mathrm{on}\ \R
        \end{cases}.
        \end{equation}
        
        \paragraph{}
        We will need to use the following local time inequality from Liqing Yan:
        
        \begin{lem}[Lemma 4.2 in \cite{Yan}]\label{local}
            Let $X$ be a continuous semimartingale with $X_0 = 0$. For $\varepsilon>0$ we define a double sequence of stopping times by $\sigma_1 = 0$, $\tau_1=\inf\{t>0 | X_t=\varepsilon\}$, $\sigma_n = \inf\{t>\tau_{n-1}|X_t=0\}$, $\tau_n=\inf\{t>\sigma_n|X_t=\varepsilon\}$. For any real function $F(\cdot)\in\mathcal{C}^2$ with $F(0)=0,\ F'(0) = 0$ and $F(\cdot) > 0$ on $(0,\varepsilon_0)$ for some $\varepsilon_0 > 0$, then for any $0<\varepsilon<\varepsilon_0$ we have
            \begin{multline*}
                0\leq L^0_t(X) \leq 2\varepsilon - \frac{2}{F(\varepsilon)}\int_0^t \theta_s(X) \left(F\left(\varepsilon\right) - \varepsilon F'\left(X_s^+\right)\right)\ \di X_s\\
                +\frac{1}{F(\varepsilon)}\int_0^t \theta_s(X)\varepsilon F''(X_s^+)\ \di[X]_s
            \end{multline*}
            with $\theta_s(X) = \sum_{n=1}^\infty \mathds{1}_{\sigma_n< s\leq \tau_n,\ 0<X_s\leq \varepsilon}(X)$.
        \end{lem}
        
        Applying lemma \ref{local} with $F(x) = x^2$, it follows:
        \begin{cor}\label{cor}
            Let X be a continuous martingale with $X_0 = 0$. With the same notations as in lemma \ref{local}, for any $\varepsilon>0$ we have
            \begin{multline}
                0\leq L^0_t(X) \leq 2\varepsilon - \frac{2}{\varepsilon}\int_0^t \theta_s(X) \left(\varepsilon - 2{X_s^+}\right)\ \di X_s
                +\frac{2}{\varepsilon}\int_0^t \theta_s(X) \ \di[X]_s
            \end{multline}
        \end{cor}
        
        \paragraph{}              
        We also recall a useful lemma concerning the solutions of (\ref{pde}) and (\ref{pde2}).
        
        \begin{lem}[Lemma 20 in \cite{Fla-Iss-Rus-2017}]\label{lem}
            Let $(\delta,p)\in K(\beta,q)$ and let $v_\lambda$ be the mild solution to (\ref{pde}) in $H_p^{1+\delta}$. Fix $\rho$ such that the integral operator is a contraction and let $\lambda>\rho$. Then $v_\lambda(t)\in\mathcal{C}^{1,\alpha}$ with $\alpha=\delta-1/p$ for each fixed $t$ and 
            \begin{equation*}
            \underset{(t,x)\in[0,T]\times\R}{\sup} |\nabla v_\lambda(t,x)| \rightarrow 0,\ as\ \lambda \rightarrow \infty
            \end{equation*}
            where the choice of $\lambda$ depends only on $\delta,\beta,\norme{b}_{H_p^{-\beta}}$, and $\norme{b}_{H_q^{-\beta}}$.
        \end{lem}
         
\begin{lem}\label{morrey} Exists $c>0$ such that for both $N\in\N$ and $\rho>1$ big enough, 
    \begin{equation}
    \begin{cases}
    \norme{u^N(t) - u(t)}_{L^\infty}\leq cKe^{\rho T}\norme{b^N-b}_{H^{-\beta}_{q}}\\        
    \norme{\nabla u^N(t) - \nabla u(t)}_{L^\infty}\leq c Ke^{\rho T}\norme{b^N-b}_{H^{-\beta}_{q}}.
    \end{cases} .
    \end{equation}
\end{lem}    

\begin{proof}
    Applying fractional Morrey inequality, $\exists c>0,\ \forall t\in[0,T]$:
    \begin{equation*}
    \begin{cases}
    \norme{u^N(t) - u(t)}_{L^\infty}\leq\norme{u^N(t) - u(t)}_{\mathcal{C}^{1,\alpha}}\leq c\norme{u^N(t)-u(t)}_{H^{1+\delta}_{p}}\\        
    \norme{\nabla u^N(t) - \nabla u(t)}_{L^\infty}\leq\norme{u^N(t) - u(t)}_{\mathcal{C}^{1,\alpha}}\leq c\norme{u^N(t)-u(t)}_{H^{1+\delta}_{p}}.
    \end{cases}        
    \end{equation*}
    
    Now, with can conclude with
    \begin{equation*}
    \norme{u^N(t)-u(t)}_{\infty,H^{1+\delta}_{p}}\leq e^{\rho T} \norme{u^N(t)-u(t)}_{\infty,H^{1+\delta}_{p}}^{(\rho)}\leq Ke^{\rho T}\norme{b^N-b}_{H^{-\beta}_{q}}
    \end{equation*} from Lemma 23 in \cite{Fla-Iss-Rus-2017}, for both $N\in\N$ and $\rho>1$ big enough, and where $\norme{f(t)}_{\infty,X}^{(\rho)} := \underset{0\leq t\leq T}{\sup} e^{-\rho t} \norme{f(t)}_X$.
\end{proof}

\begin{theo}
    Let $f$ be $\mu$-Hölder with constant $C_f>0$ and $\mu\in(0,1]$. Then for $N\in\N$, $\rho>1$, $\lambda$ big enough, exists $\xi_f$ independent of $N$ such that:
    
    \begin{equation*}
    \E\left[f\left(X_T\right)-f\left(X_T^N\right)\right] \leq \xi_f \norme{b^N-b}_{H^{-\beta}_{q}}^\gamma
    \end{equation*}
    
\end{theo}

\begin{proof}
    By Lemma \ref{lem}, we choose $\lambda$ big enough for $\nabla u$ and $\nabla u^N$ to be bounded by $\frac{1}{2}$. $\lambda$ can be chosen independently of $N$ as far as $\norme{b - b^N}_{H_q^s(\R)} \underset{N\rightarrow\infty}{\longrightarrow} 0$ (See Step 2 of the proof of Proposition 29 in \cite{Fla-Iss-Rus-2017}). Therefore $u^N$ and $u$ are $\frac{1}{2}$-lipschitz. We recall that in this case, by Lemma 22 in \cite{Fla-Iss-Rus-2017}, $\Psi(t,\cdot)$ is 2-lipschitz.
    
    \begin{equation*}
    \E\left[f\left(X_T\right)-f\left(X_T^N\right)\right] = \E\left[f\left(\Psi\left(T,Y_T\right)\right)-f\left(\Psi\left(T,Y_T^N\right)\right)\right]
    \end{equation*}
    
    \begin{equation*}
    \leq 2^\mu C_f  \E\left[\left|Y_T-Y_T^N\right|^\mu\right] \leq 2^\mu C_f  \E\left[\left|Y_T-Y_T^N\right|\right]^\mu
    \end{equation*}
    
    by Jensen's inequality. Let $t\in[0,T]$.
    \begin{multline*}
    Y_t-Y_t^N = (\lambda + 1 )\int_0^t\left\{u\left(s,\Psi\left(s,Y_s\right)\right)-u^N\left(s,\Psi\left(s,Y_s^N\right)\right)\right\}\ \di s\\ + \int_0^t\left\{\nabla u\left(s,\Psi\left(s,Y_s\right)\right)-\nabla u^N\left(t,\Psi\left(s,Y_s^N\right)\right)\right\}\ \di W_s.
    \end{multline*}
    
    \begin{rem}  
    For clarity purpose, we note $\tilde{u}\left(s,x\right) = u\left(s,\Psi\left(s,x\right)\right)$ and use the same notation for the gradient and the approximated mild solution. We can notice that $\tilde{u}$ is 1-lipschitz in space and $\nabla\tilde{u}$ is $\alpha$-Hölder with constant $2\norme{u}_{\mathcal{C}^{1,\alpha}}$.
    \end{rem}
    
    We apply Meyer-Tanaka's formula to obtain:
    \begin{multline*}
    \left|Y_t-Y_t^N\right| = (\lambda + 1)\ \int_0^t\mathrm{sign}(Y_s-Y_s^N)\left\{\tilde{u}\left(s,Y_s\right)-\tilde{u}^N\left(s,Y_s^N\right)\right\}\ \di s\\ + \int_0^t\mathrm{sign}(Y_s-Y_s^N)\left\{\nabla\tilde{u}\left(s,Y_s\right)-\nabla\tilde{u}^N\left(s,Y_s^N\right)\right\}\ \di W_s + L_t^0(Y-Y^N).
    \end{multline*}
    
    Taking the expectation leads to:
    \begin{multline*}
    \E\left[\left|Y_t-Y_t^N\right|\right] = (\lambda + 1)\ \E\left[\int_0^t\mathrm{sign}(Y_s-Y_s^N)\left\{\tilde{u}\left(s,Y_s\right)-\tilde{u}^N\left(s,Y_s^N\right)\right\}\ \di s\right]\\ + \E \left[L_t^0(Y-Y^N)\right]
    \end{multline*}
    because $\nabla\tilde{u}$ and $\nabla\tilde{u}^N$ are bounded so the Itô integral is a martingale.    
    \begin{multline*}
    \E\left[\left|Y_t-Y_t^N\right|\right]\leq (\lambda + 1)\ \E\left[\int_0^t\left\{\tilde{u}\left(s,Y_s\right)-\tilde{u}\left(s,Y_s^N\right)\right\} \di s\right]\\ +(\lambda + 1)\ \E\left[\int_0^t\left\{\tilde{u}\left(s,Y_s^N\right)-\tilde{u}^N\left(s,Y_s^N\right)\right\} \di s\right] + \E \left[L_t^0(Y-Y^N)\right].
    \end{multline*}
    
    \paragraph{}
    We use Lemma \ref{morrey} and the 1-lipschitz property of $\tilde{u}$:
    \begin{multline*}
    \E\left[\left|Y_t-Y_t^N\right|\right]\leq (\lambda + 1)\ \E\left[\int_0^t\left|Y_s-Y_s^N\right| \di s\right] +(\lambda + 1)\ ctKe^{\rho T}\norme{b^N-b}_{H^{-\beta}_{q}}\\ + \E \left[L_t^0(Y-Y^N)\right]
    \end{multline*}
    
    \begin{multline*}
    \leq (\lambda + 1)\ \int_0^t\E\left[\left|Y_s-Y_s^N\right|\right] \di s + (\lambda + 1)\ cTKe^{\rho T}\norme{b^N-b}_{H^{-\beta}_{q}}\\ + \E \left[L_T^0(Y-Y^N)\right].
    \end{multline*}
    where we have used the fact that $L_t^0(Y-Y^N)$ is an increasing process.
    
    \paragraph{}
    By Gronwall's Lemma, it follows:
    \begin{equation}
    \E\left[\left|Y_T-Y_T^N\right|\right] \leq C(N) \exp((\lambda+1)T)
    \end{equation}
    with $C(N) = (\lambda + 1)\ cTKe^{\rho T}\norme{b^N-b}_{H^{-\beta}_{q}} + \E \left[L_T^0(Y-Y^N)\right].$      
    
    \paragraph{}
    We now have to study the term $\E \left[L_T^0(Y-Y^N)\right]$. Let $\varepsilon>0$. Corollary \ref{cor} gives us:    
    \begin{multline*}
    0\leq L^0_T(Y-Y^N) \leq 2\varepsilon - \frac{2}{\varepsilon}\int_0^T \theta_s(Y-Y^N) \left(\varepsilon - 2{(Y_s-Y_s^N)^+}\right)\ \di (Y_s-Y_s^N)\\
    +\frac{2}{\varepsilon}\int_0^T \theta_s(Y-Y^N) \ \di[(Y-Y^N)]_s.
    \end{multline*}    
    \begin{rem}
        Note that $\theta_s(Y-Y^N) \left|\varepsilon - 2{(Y_s-Y_s^N)^+}\right|\leq \varepsilon\theta_s(Y-Y^N)$.
    \end{rem}
We take the expectation to remove again the martingale part:
    \begin{multline*}
    \E\left[L^0_T(Y-Y^N)\right] \leq 2\varepsilon + 2(\lambda + 1)\ \E\left[\int_0^T\theta_s(Y-Y^N)\left\{\tilde{u}\left(s,Y_s\right)-\tilde{u}^N\left(s,Y_s^N\right)\right\} \di s \right]\\
    +\frac{2}{\varepsilon}\ \E\left[\int_0^T \theta_s(Y-Y^N)\left\{\nabla\tilde{u}\left(s,Y_s\right)-\nabla\tilde{u}^N\left(s,Y_s^N\right)\right\}^2\ \di s\right]
    \end{multline*}
    \begin{multline*}
    \leq 2\varepsilon + 2(\lambda + 1)\ \E\left[\int_0^T\theta_s(Y-Y^N)\left\{\tilde{u}\left(s,Y_s\right)-\tilde{u}\left(s,Y_s^N\right)\right\}\ \di s \right]\\+2(\lambda + 1)\ \E\left[\int_0^T\theta_s(Y-Y^N)\left\{\tilde{u}\left(s,Y_s^N\right)-\tilde{u}^N\left(s,Y_s^N\right)\right\}\ \di s \right]\\
    +\frac{4}{\varepsilon}\ \E\left[\int_0^T \theta_s(Y-Y^N)\left\{\nabla\tilde{u}\left(s,Y_s\right)-\nabla\tilde{u}\left(s,Y_s^N\right)\right\}^2\ \di s\right]\\
    +\frac{4}{\varepsilon}\ \E\left[\int_0^T \theta_s(Y-Y^N)\left\{\nabla\tilde{u}\left(s,Y_s^N\right)-\nabla\tilde{u}^N\left(s,Y_s^N\right)\right\}^2\ \di s\right]
    \end{multline*}
    \begin{multline*}
    \leq 2\varepsilon + 2(\lambda + 1)\ \E\left[\int_0^T\theta_s(Y-Y^N)\left|Y_s - Y^N_s\right|\ \di s \right] \\
    +\frac{16\norme{u}_{\mathcal{C}^{1,\alpha}}^2}{\varepsilon}\ \E\left[\int_0^T \theta_s(Y-Y^N)\left|Y_s - Y^N_s\right|^{2\alpha}\ \di s\right]\\
    +2(\lambda + 1)\ cT Ke^{\rho T}\norme{b^N-b}_{H^{-\beta}_{q}} +4c^2T K^2e^{2\rho T}\norme{b^N-b}_{H^{-\beta}_{q}}^2\varepsilon^{-1}
    \end{multline*}
    by Lemma \ref{morrey}, the 1-lipschitz property of $\tilde{u}$ and the $\alpha$-Hölder property of $\nabla\tilde{u}$ (with constant $2\norme{u}_{\mathcal{C}^{1,\alpha}}$).
    
    \begin{multline*}
    \E\left[L^0_T(Y-Y^N)\right]\leq 2(\lambda + 1)\ cT Ke^{\rho T}\norme{b^N-b}_{H^{-\beta}_{q}} + 16\norme{u}_{\mathcal{C}^{1,\alpha}}^2 T \varepsilon^{2\alpha-1}\\
    + 2(1+(\lambda + 1)T)\varepsilon + 4c^2T K^2e^{2\rho T}\norme{b^N-b}_{H^{-\beta}_{q}}^2\varepsilon^{-1}
    \end{multline*}
    
    We choose an optimal $\varepsilon$.
    
\end{proof}   
 
\bibliographystyle{abbrv}
\bibliography{pre}
    
\end{document}